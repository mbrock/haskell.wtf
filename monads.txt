WTF are monads?
---------------

Excellent question!

Unfortunately, this text will probably not help very much.

Instead, I'm going to try to clarify why it is that the concept is
both difficult to understand and easy to use -- and why you should let
go of your worry and go on writing programs even without the kind of
understanding you imagine that you need.

There's a basic problem with the way people often approach the "monad"
concept, and it causes unnecessary anxiety and confusion. It has to do
with how we talk about /abstract concepts/ and how we desire to
"understand" them in a way that's neither realistic nor necessary.

Target audience: people learning Haskell who are confused about the
notion of monads, who want to understand enough to let them write
programs and a reason not to be ashamed of being confused.

                    1. USING WITHOUT UNDERSTANDING

Maybe I'm not actually qualified to write this kind of article, since
I don't "understand" what monads "really are." Yet oddly enough I'm
capable of writing Haskell code. I've even done it professionally.

It's not because I ignore monads. I use them routinely; I write
functions that work on general monads; sometimes I even implement my
own -- all without having much of an intuitive understanding of what
monads really are.

That might seem weird. Am I just guessing randomly?

Well, sometimes.

Mostly I just use my practical experience, the help of the type
checker, some bits of intuition here and there, analogy, and whatever
cognitive tools I find lying around my mental toolbox. I get by with a
little help from my friends.


          2. ABSTRACT CONCEPTS AND THE DESIRE FOR INTUITION

Why is it that we can work with monads productively without
understanding what they really are deep down? It's because the concept
"monad" is an abstraction. If we work on a more concrete level, we can
benefit from the concept without having a deep understanding of it.

When I'm aware of what level of abstraction I'm working at, I find
myself less confused. Since I know that the monad concept is very
abstract, I know that if things get too fuzzy, I can choose to think
in more concrete terms.

"Monad" has a formal definition that you can look up. There: now you
know. But that's unlikely to help with your intuitive understanding.

Intuitive understanding is what people seem to want, which is why
there are so many articles explaining how monads are kind of like
conveyor belts, or kind of like burritos.

Students get bored with abstractions and imagine that what they need
is an intuition, some way of visualizing the abstract concept to make
it clear in their minds. But thinking of monads as burritos doesn't
help me write code, it just makes me hungry.

Intuition is overrated when it comes to abstract concepts.

Or, I should say, we tend to try to build intuition in ways that don't
quite work, and this is frustrating, as I remember quite well from the
abstract algebra course I failed.

The upshot is that I believe my experience with Haskell might help me
with learning about abstract algebra in the future. I'd spend more
time practicing the basic operations, looking at the proofs, learning
to manipulate expressions confidently... I'd keep in mind that
abstract algebra is really no more difficult than arithmetic. And less
time drinking too much coffee while trying to conjure up some mystical
intuition of what an Abelian group really is.


             3. EXAMPLES OF ABSTRACTION IN MODERN ALGEBRA




                              OLD STUFF

Working with abstract formal concepts is a trade skill of
mathematicians. They value abstractions because it increases the
leverage of mathematical work. If you know that the thing you're
working on can be viewed as an instance of some general concept, then
you can use more stuff constructed by other mathematicians. It's like
"code reuse."

For example, modern algebraists have constructed generalizations with
names like "group," "ring," and "field." They've turned out to be
fruitful notions, leading to advances in cryptography and geometry and
computation and all kinds of good stuff. In a way they are extremely
simple concepts, and you can look up their definitions.

I'll explain a bit about how they work in a hand-wavey way for those
who haven't studied abstract algebra. I took a class on it years ago
but I lost focus and ended up not finishing it. It was interesting
though, and kind of fun.

If you're already a bit familiar with basic stuff in abstract algebra,
this is just repetition to set the context.

Basically, a bunch of mathematicians from different fields noted that
they were saying similar things. Their proofs had the same "shapes."
So they wondered if if it would be possible to use common proofs and
concepts while maintaining mathematical rigor.

They were fields like number theory; the study of vectors; symbolic
algebra; geometrical studies of symmetries and rotations; boolean
algebra; linear algebra; and so on. While many of their proofs and
results were specific to their fields, they all relied on
common notions.

Out of the wish to make these common notions exact came the
formalizations of modern algebra. I'll give three examples of such
common notions and then explain how taken together they allow for
proofs of general theorems that can then be applied in
different fields.

First is the notion of an "identity value." That means a value that
has no effect on a calculation, like zero when you're adding numbers;
one when you're multiplying; the 360 degree rotation of a figure; or
the passing of 24 hours on an analog watch. For example

                          x + 0 = 0 + x = x

or

                          x * 1 = 1 * x = 1.

Second, the notion of an "inverse value." That's a value that cancels
out some other given value in some kind of calculation, like

                             x + (-x) = 0

or

                            y * (1/y) = 1.

Third, the notion of an "associative operator." That's an algebraic
operator whose outcome doesn't depend on the order in which you apply
it when you have many operands, for example:

                      (x + y) + z = x + (y + z)

We can now write algebraic proofs that refer to only these abstract
notions.

For example, I've discovered this astounding formula:

                           (x + y) - y = x.

I can prove this using basic high school algebra, and even if I don't
know about abstract algebra, I will still only be referring to these
three abstract notions of identity value, inverse value, and
associativity.

Here are the steps:

Associativity lets us rewrite the formula to

                           x + (y - y) = x

which is the same as

                         x + (y + (-y)) = x.

By the definition of -y as the additive inverse of y we get

                              x + 0 = x

which by definition of 0 as the additive identity reduces to

                                x = x

which seems pretty likely.

Given this proof, anyone using an associative operator with inverses
and an identity can be confident that my astounding formula is
correct, since the proof steps make no further assumptions.








<!-- Abstract concepts are abstract and difficult if not impossible -->
<!-- to understand. It's unfortunate that people learning Haskell -->
<!-- sometimes believe that they have to understand them. Especially -->
<!-- the infamous monads. They don't!  Next question! -->

<!-- ... -->

<!-- Why then do people talk about monads, and why does the word -->
<!-- show up in nearly all Haskell programs? Excellent questions! -->

<!-- Here's an example of a comparable situation. -->

<!-- You probably sometimes program using numbers and operations -->
<!-- like addition, subtraction, multiplication, and division. -->

<!-- You might know that modern math talks about these notions using -->
<!-- the abstract algebraic concepts: groups, rings, and fields. -->

<!-- But you didn't learn about those abstract concepts before you -->
<!-- were experienced with actual arithmetic. You wouldn't have -->
<!-- cared, and you wouldn't feel like you were "understanding" -->
<!-- anything except a lot of abstract definitions and theorems. -->

<!-- So, the concept "monad" is something kind of like the concept -->
<!-- "group." It's the outcome of modern mathematical -->
<!-- research, and it's an abstraction. -->

<!-- Haskell is all about blending the abstract and the concrete. It -->
<!-- builds on concepts from algebra and type theory and category -->
<!-- theory and lots of clever stuff. That's one reason why it -->
<!-- generally makes sense even though it sometimes seems obscure. -->
